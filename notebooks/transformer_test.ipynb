{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.LogTransformer import Transformer\n",
    "from ml_collections import ConfigDict\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import jax\n",
    "from utils.single_gpu import Batch, TrainState, accumulate_gradients, print_metrics\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any, Dict, Tuple\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]\n",
    "import functools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = ConfigDict(\n",
    "    dict(\n",
    "        batch_size=64,\n",
    "        seq_len=512,\n",
    "        vocab_size=512//2,\n",
    "    )\n",
    ")\n",
    "model_config = ConfigDict(\n",
    "    dict(\n",
    "        hidden_size=1024,\n",
    "        dropout_rate=0.1,\n",
    "        mlp_expansion=4,\n",
    "        num_layers=12,\n",
    "        head_dim=128,\n",
    "        causal_mask=True,\n",
    "        max_seq_len=data_config.seq_len,\n",
    "        vocab_size=data_config.vocab_size,\n",
    "        num_outputs=data_config.vocab_size,\n",
    "        dtype=jnp.bfloat16,\n",
    "        out_dtype=jnp.float32,\n",
    "        softmax_dtype=jnp.float32,\n",
    "        scan_layers=True,\n",
    "        #remat=(\"\"),\n",
    "        remat=(\"MLP\", \"Attn\"),\n",
    "        determinant=True,\n",
    "    )\n",
    ")\n",
    "model_config.num_heads = model_config.hidden_size // model_config.head_dim\n",
    "optimizer_config = ConfigDict(\n",
    "    dict(\n",
    "        learning_rate=4e-4,\n",
    "        num_minibatches=4,\n",
    "    )\n",
    ")\n",
    "config = ConfigDict(\n",
    "    dict(\n",
    "        model=model_config,\n",
    "        optimizer=optimizer_config,\n",
    "        data=data_config,\n",
    "        seed=42,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config=config.model)\n",
    "optimizer = optax.adam(\n",
    "    learning_rate=optax.warmup_exponential_decay_schedule(\n",
    "        init_value=0,\n",
    "        peak_value=config.optimizer.learning_rate,\n",
    "        warmup_steps=10,\n",
    "        transition_steps=1,\n",
    "        decay_rate=0.99,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = jax.random.randint(\n",
    "    jax.random.PRNGKey(0),\n",
    "    (config.data.batch_size, config.data.seq_len),\n",
    "    1,\n",
    "    config.data.vocab_size,\n",
    ")\n",
    "batch_transformer = Batch(\n",
    "    # 把第一个元素设成0，其他的后移，再把最后一个元素扔掉\n",
    "    inputs=jnp.pad(tokens[:, :-1], ((0, 0), (1, 0)), constant_values=0),\n",
    "    labels=tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transformer.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rng, state_rng = jax.random.split(jax.random.PRNGKey(config.seed))\n",
    "params = model.init(\n",
    "    model_rng,\n",
    "    batch_transformer.inputs[: config.data.batch_size // config.optimizer.num_minibatches],\n",
    "    train=False,\n",
    ")[\"params\"]\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    "    rng=state_rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = state.apply_fn({\"params\": state.params}, batch_transformer.inputs, train=False, rngs={\"dropout\": state.rng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "from netket import jax as nkjax\n",
    "\n",
    "M = jax.random.normal(jax.random.PRNGKey(0), (4,4))\n",
    "print(M.dtype)\n",
    "A = nkjax.logdet_cmplx(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 155_877_376\n"
     ]
    }
   ],
   "source": [
    "def get_num_params(state: TrainState) -> int:\n",
    "    return sum(np.prod(x.shape) for x in jax.tree_util.tree_leaves(state.params))\n",
    "\n",
    "\n",
    "print(f\"Number of parameters: {get_num_params(state):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 699,  693,  193, ...,  141, 1855,  430],\n",
       "       [1930, 1630,  811, ..., 1884, 1227,  469],\n",
       "       [ 521,   32, 1572, ..., 1088, 1466, 2023],\n",
       "       ...,\n",
       "       [1312, 1531,  948, ...,  114,  988, 1095],\n",
       "       [1387,  462, 2036, ..., 1100, 1100,  700],\n",
       "       [ 680, 1930,  898, ..., 1259, 1643, 1876]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_pred_loss(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Next token prediction loss function.\"\"\"\n",
    "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": rng})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = np.prod(batch.labels.shape)\n",
    "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit,\n",
    "    donate_argnames=(\n",
    "        \"state\",\n",
    "        \"metrics\",\n",
    "    ),\n",
    ")\n",
    "def train_step_transformer(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    \"\"\"Training step function.\n",
    "\n",
    "    Executes a full training step with gradient accumulation for the next-token prediction task.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        metrics: Current metrics, accumulated from previous training steps.\n",
    "        batch: Training batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with updated training state (parameters, optimizer state, etc.) and metrics.\n",
    "    \"\"\"\n",
    "    # Split the random number generator for the current step.\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    # Determine gradients and metrics for the full batch.\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=next_token_pred_loss,\n",
    "        use_scan=True,\n",
    "    )\n",
    "    # Optimizer step.\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Accumulate metrics across training steps.\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_transformer,\n",
    "    state,\n",
    "    None,\n",
    "    batch_transformer,\n",
    ")\n",
    "metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final metrics - Transformer \n",
      "accuracy: 0.000641\n",
      "loss: 7.778461\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(4)):\n",
    "    state, metrics = train_step_transformer(state, metrics, batch_transformer)\n",
    "final_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state, final_metrics = train_step_transformer(state, final_metrics, batch_transformer)\n",
    "print_metrics(final_metrics, \"Final metrics - Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': (ShapeDtypeStruct(shape=(), dtype=int64), ShapeDtypeStruct(shape=(), dtype=int64)), 'loss': (ShapeDtypeStruct(shape=(), dtype=float32), ShapeDtypeStruct(shape=(), dtype=int64))}\n",
      "(Array(0., dtype=float32), Array(0, dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(metric_shapes)\n",
    "print(metrics[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = state.apply_fn({\"params\": state.params}, batch_transformer.inputs, train=False, rngs={\"dropout\": state.rng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 512, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transformer.inputs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
